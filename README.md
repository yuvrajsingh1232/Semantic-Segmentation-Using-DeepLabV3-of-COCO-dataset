
# Semantic Segmentation Using DeepLabV3 on the COCO Dataset

This repository implements semantic segmentation using DeepLabV3 with a MiT-B2 encoder, trained on a processed subset of the COCO dataset. It involves dataset preparation from COCO annotations and PyTorch Lightning-based training.

## ğŸ“¦ Task 1: Dataset Preparation (COCO to Masks)

The script data_preprocessing.py converts COCO annotations into segmentation masks.

### âœ… Key Features
- Converts COCO annotations to multi-class masks.
- Handles overlapping objects using per-pixel max logic.
- Skips crowd annotations and invalid entries.
- Processes up to 5,000 images from the COCO dataset.
- Outputs:
  - RGB images to output/images/
  - Grayscale class masks to output/masks/

### â–¶ Usage
bash
python data_preprocessing.py \
  --annotations path/to/instances_train2017.json \
  --images path/to/train2017 \
  --output path/to/output_dir \
  --max-images 5000

## ğŸ§  Task 2: Model Training (DeepLabV3 + MiT-B2)

The training pipeline uses PyTorch Lightning and segmentation_models_pytorch:
- *Architecture*: DeepLabV3+ with MiT-B2 encoder.
- *Loss*: CrossEntropyLoss (with class weights).
- *Metrics*: IoU (Jaccard), Dice Coefficient.
- *Logging*: WandB, mixed precision training.
- *Additional Features*: Early stopping and model checkpointing.

### ğŸ— Architecture Highlights
- *DeepLabV3Plus* from segmentation_models_pytorch.
- *Encoder*: mit_b2, pretrained on ImageNet.
- *Image Size*: Resized to 512Ã—512.
- *Augmentations*: Applied using albumentations.

### â–¶ Usage
bash
python train.py \
  --data_path path/to/output_dir \
  --epochs 50 \
  --lr 1e-4 \
  --num_classes 80 \
  --batch_size 16 \
  --img_size 512,512


## ğŸ§ª Metrics

- *IoU (Jaccard Index)* per class and overall.
- *Dice Coefficient* averaged across classes.
- *Pixel Accuracy* (optional via WandB logging).

---

## ğŸ§° Environment Setup

To ensure reproducibility, we use [uv](https://github.com/astral-sh/uv) for environment management.

### âœ… Install dependencies
bash
pip install uv
uv venv
uv pip install -r requirements.txt
---

## ğŸ—ƒ Project Structure

- data_preprocessing.py â€” Script to convert COCO annotations to segmentation masks.
- train.py â€” PyTorch Lightning training script using DeepLabV3+ with MiT-B2 encoder.
- trainning.ipynb â€” Jupyter notebook for visualizing dataset and predictions.
- requirements.txt â€” Contains all dependencies to set up the environment using uv.
- results/ â€” Folder containing sample inputs, ground truth masks, and predicted masks.
  - input1.jpg â€” Sample input image.
  - mask1.png â€” Corresponding ground truth mask.
  - pred1.png â€” Model prediction for the input.
- checkpoints/ â€” Directory where trained model weights are saved.
- README.md â€” Project overview, setup instructions, and usage examples.

## âœ¨ Highlights

- ğŸ”„ *End-to-End Pipeline*: From COCO annotations to trained segmentation model.
- ğŸ–¼ *Multi-class Mask Generation*: Converts COCO annotations into pixel-wise labeled masks.
- ğŸ’¡ *Efficient Model*: Uses DeepLabV3+ with MiT-B2 encoder for strong performance and speed.
- âš™ *Training Framework*: Built with PyTorch Lightning for modularity and scalability.
- ğŸ¯ *Evaluation Metrics*: Includes IoU, Dice Coefficient, and optional Pixel Accuracy.
- ğŸ“Š *WandB Integration*: Logs training metrics and visualizations in real-time.
- ğŸ§ª *Reproducibility*: Uses uv for environment isolation and dependency management.
- ğŸ“ *Well-Structured Codebase*: Easy to modify and extend for new datasets or encoders.
- ğŸ›  *Robust Preprocessing*: Handles edge cases, invalid entries, and overlapping masks.
- ğŸ” *Visualization Ready*: Includes sample inputs, masks, and predictionsÂ inÂ results/.
